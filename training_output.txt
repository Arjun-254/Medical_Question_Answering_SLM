Iter 1: Val loss 2.247, Val took 4.935s
Iter 10: Train loss 2.416, Learning Rate 1.000e-05, It/sec 3.374, Tokens/sec 875.431, Trained Tokens 2595, Peak mem 5.258 GB
Iter 20: Train loss 2.365, Learning Rate 1.000e-05, It/sec 2.583, Tokens/sec 757.246, Trained Tokens 5527, Peak mem 6.076 GB
Iter 30: Train loss 2.262, Learning Rate 1.000e-05, It/sec 2.183, Tokens/sec 804.974, Trained Tokens 9214, Peak mem 7.665 GB
Iter 40: Train loss 2.306, Learning Rate 1.000e-05, It/sec 3.489, Tokens/sec 750.558, Trained Tokens 11365, Peak mem 7.665 GB
Iter 50: Train loss 2.138, Learning Rate 1.000e-05, It/sec 2.419, Tokens/sec 787.714, Trained Tokens 14621, Peak mem 8.561 GB
Iter 60: Train loss 2.298, Learning Rate 1.000e-05, It/sec 2.407, Tokens/sec 791.520, Trained Tokens 17909, Peak mem 8.561 GB
Iter 70: Train loss 2.269, Learning Rate 1.000e-05, It/sec 3.618, Tokens/sec 735.236, Trained Tokens 19941, Peak mem 8.561 GB
Iter 80: Train loss 2.074, Learning Rate 1.000e-05, It/sec 1.333, Tokens/sec 764.902, Trained Tokens 25679, Peak mem 15.838 GB
Iter 90: Train loss 2.140, Learning Rate 1.000e-05, It/sec 3.154, Tokens/sec 734.029, Trained Tokens 28006, Peak mem 15.838 GB
Iter 100: Train loss 2.162, Learning Rate 1.000e-05, It/sec 1.895, Tokens/sec 771.584, Trained Tokens 32077, Peak mem 15.838 GB
Iter 110: Train loss 2.035, Learning Rate 1.000e-05, It/sec 1.953, Tokens/sec 743.841, Trained Tokens 35886, Peak mem 15.838 GB
Iter 120: Train loss 2.057, Learning Rate 1.000e-05, It/sec 1.754, Tokens/sec 740.555, Trained Tokens 40107, Peak mem 15.838 GB
Iter 130: Train loss 2.156, Learning Rate 1.000e-05, It/sec 2.466, Tokens/sec 706.133, Trained Tokens 42970, Peak mem 15.838 GB
Iter 140: Train loss 1.891, Learning Rate 1.000e-05, It/sec 2.754, Tokens/sec 715.894, Trained Tokens 45569, Peak mem 15.838 GB
Iter 150: Train loss 1.997, Learning Rate 1.000e-05, It/sec 1.839, Tokens/sec 746.935, Trained Tokens 49630, Peak mem 15.838 GB
Iter 160: Train loss 2.064, Learning Rate 1.000e-05, It/sec 3.170, Tokens/sec 672.094, Trained Tokens 51750, Peak mem 15.838 GB
Iter 170: Train loss 1.961, Learning Rate 1.000e-05, It/sec 2.663, Tokens/sec 717.066, Trained Tokens 54443, Peak mem 15.838 GB
Iter 180: Train loss 1.962, Learning Rate 1.000e-05, It/sec 2.128, Tokens/sec 736.339, Trained Tokens 57903, Peak mem 15.838 GB
Iter 190: Train loss 1.931, Learning Rate 1.000e-05, It/sec 2.188, Tokens/sec 676.299, Trained Tokens 60994, Peak mem 15.838 GB
Iter 200: Val loss 1.813, Val took 7.531s
Iter 200: Train loss 1.913, Learning Rate 1.000e-05, It/sec 16.713, Tokens/sec 4049.609, Trained Tokens 63417, Peak mem 15.838 GB
Iter 210: Train loss 1.886, Learning Rate 1.000e-05, It/sec 2.012, Tokens/sec 729.840, Trained Tokens 67045, Peak mem 15.838 GB
Iter 220: Train loss 1.764, Learning Rate 1.000e-05, It/sec 1.374, Tokens/sec 740.225, Trained Tokens 72434, Peak mem 15.838 GB
Iter 230: Train loss 1.870, Learning Rate 1.000e-05, It/sec 1.705, Tokens/sec 746.101, Trained Tokens 76811, Peak mem 16.150 GB
Iter 240: Train loss 1.713, Learning Rate 1.000e-05, It/sec 2.836, Tokens/sec 698.517, Trained Tokens 79274, Peak mem 16.150 GB
Iter 250: Train loss 1.851, Learning Rate 1.000e-05, It/sec 3.123, Tokens/sec 672.304, Trained Tokens 81427, Peak mem 16.150 GB
Iter 260: Train loss 1.776, Learning Rate 1.000e-05, It/sec 2.557, Tokens/sec 722.171, Trained Tokens 84251, Peak mem 16.150 GB
Iter 270: Train loss 1.912, Learning Rate 1.000e-05, It/sec 2.728, Tokens/sec 703.030, Trained Tokens 86828, Peak mem 16.150 GB
Iter 280: Train loss 1.918, Learning Rate 1.000e-05, It/sec 2.495, Tokens/sec 709.386, Trained Tokens 89671, Peak mem 16.150 GB
Iter 290: Train loss 1.900, Learning Rate 1.000e-05, It/sec 2.762, Tokens/sec 713.351, Trained Tokens 92254, Peak mem 16.150 GB
Iter 300: Train loss 1.646, Learning Rate 1.000e-05, It/sec 1.897, Tokens/sec 713.071, Trained Tokens 96012, Peak mem 16.150 GB
Iter 310: Train loss 1.891, Learning Rate 1.000e-05, It/sec 3.256, Tokens/sec 676.359, Trained Tokens 98089, Peak mem 16.150 GB
Iter 320: Train loss 1.947, Learning Rate 1.000e-05, It/sec 1.705, Tokens/sec 746.348, Trained Tokens 102466, Peak mem 16.150 GB
Iter 330: Train loss 1.762, Learning Rate 1.000e-05, It/sec 2.431, Tokens/sec 699.793, Trained Tokens 105345, Peak mem 16.150 GB
Iter 340: Train loss 1.818, Learning Rate 1.000e-05, It/sec 2.028, Tokens/sec 714.261, Trained Tokens 108867, Peak mem 16.150 GB
Iter 350: Train loss 1.718, Learning Rate 1.000e-05, It/sec 2.474, Tokens/sec 696.319, Trained Tokens 111681, Peak mem 16.150 GB
Iter 360: Train loss 1.784, Learning Rate 1.000e-05, It/sec 2.644, Tokens/sec 703.284, Trained Tokens 114341, Peak mem 16.150 GB
Iter 370: Train loss 1.796, Learning Rate 1.000e-05, It/sec 2.955, Tokens/sec 687.618, Trained Tokens 116668, Peak mem 16.150 GB
Iter 380: Train loss 1.746, Learning Rate 1.000e-05, It/sec 1.878, Tokens/sec 717.931, Trained Tokens 120491, Peak mem 16.150 GB
Iter 390: Train loss 1.759, Learning Rate 1.000e-05, It/sec 2.429, Tokens/sec 695.775, Trained Tokens 123355, Peak mem 16.150 GB
Iter 400: Val loss 1.708, Val took 7.605s
Iter 400: Train loss 1.923, Learning Rate 1.000e-05, It/sec 19.263, Tokens/sec 6526.467, Trained Tokens 126743, Peak mem 16.150 GB
Iter 410: Train loss 1.741, Learning Rate 1.000e-05, It/sec 2.045, Tokens/sec 710.756, Trained Tokens 130219, Peak mem 16.150 GB
Iter 420: Train loss 1.723, Learning Rate 1.000e-05, It/sec 2.236, Tokens/sec 707.978, Trained Tokens 133385, Peak mem 16.150 GB
Iter 430: Train loss 1.651, Learning Rate 1.000e-05, It/sec 3.412, Tokens/sec 651.688, Trained Tokens 135295, Peak mem 16.150 GB
Iter 440: Train loss 1.686, Learning Rate 1.000e-05, It/sec 1.949, Tokens/sec 708.494, Trained Tokens 138930, Peak mem 16.150 GB
Iter 450: Train loss 1.702, Learning Rate 1.000e-05, It/sec 1.756, Tokens/sec 710.647, Trained Tokens 142976, Peak mem 16.150 GB
Iter 460: Train loss 1.818, Learning Rate 1.000e-05, It/sec 2.607, Tokens/sec 715.822, Trained Tokens 145722, Peak mem 16.150 GB
Iter 470: Train loss 1.653, Learning Rate 1.000e-05, It/sec 1.792, Tokens/sec 724.201, Trained Tokens 149764, Peak mem 16.150 GB
Iter 480: Train loss 1.623, Learning Rate 1.000e-05, It/sec 2.255, Tokens/sec 723.294, Trained Tokens 152972, Peak mem 16.150 GB
Iter 490: Train loss 1.763, Learning Rate 1.000e-05, It/sec 3.003, Tokens/sec 661.257, Trained Tokens 155174, Peak mem 16.150 GB
Iter 500: Train loss 1.643, Learning Rate 1.000e-05, It/sec 1.557, Tokens/sec 723.193, Trained Tokens 159820, Peak mem 16.150 GB
Iter 510: Train loss 1.787, Learning Rate 1.000e-05, It/sec 2.172, Tokens/sec 716.773, Trained Tokens 163120, Peak mem 16.150 GB
Iter 520: Train loss 1.905, Learning Rate 1.000e-05, It/sec 2.721, Tokens/sec 678.084, Trained Tokens 165612, Peak mem 16.150 GB
Iter 530: Train loss 1.813, Learning Rate 1.000e-05, It/sec 1.384, Tokens/sec 737.174, Trained Tokens 170940, Peak mem 16.150 GB
Iter 540: Train loss 1.619, Learning Rate 1.000e-05, It/sec 3.283, Tokens/sec 670.066, Trained Tokens 172981, Peak mem 16.150 GB
Iter 550: Train loss 1.694, Learning Rate 1.000e-05, It/sec 2.785, Tokens/sec 691.572, Trained Tokens 175464, Peak mem 16.150 GB
Iter 560: Train loss 1.765, Learning Rate 1.000e-05, It/sec 2.583, Tokens/sec 661.383, Trained Tokens 178025, Peak mem 16.150 GB
Iter 570: Train loss 1.802, Learning Rate 1.000e-05, It/sec 2.476, Tokens/sec 713.719, Trained Tokens 180908, Peak mem 16.150 GB
Iter 580: Train loss 1.698, Learning Rate 1.000e-05, It/sec 2.564, Tokens/sec 696.946, Trained Tokens 183626, Peak mem 16.150 GB
Iter 590: Train loss 1.689, Learning Rate 1.000e-05, It/sec 1.723, Tokens/sec 699.975, Trained Tokens 187688, Peak mem 16.150 GB
Iter 600: Val loss 1.637, Val took 7.010s
Iter 600: Train loss 1.547, Learning Rate 1.000e-05, It/sec 19.317, Tokens/sec 4833.179, Trained Tokens 190190, Peak mem 16.150 GB
